# ML Classification Models Dashboard

A comprehensive machine learning classification project that implements 6 different classification algorithms and provides an interactive Streamlit web application for model evaluation and comparison.

## ğŸ“Œ Problem Statement

The objective of this project is to build and evaluate multiple machine learning classification models on a chosen dataset and deploy an interactive web application for demonstration. This project covers the complete end-to-end ML deployment workflow: data preprocessing, model training, evaluation, UI design, and cloud deployment.

The application allows users to:
- Upload their own classification datasets
- Select and train different ML models
- View comprehensive evaluation metrics
- Compare model performances visually

## ğŸ“Š Dataset Description

**Dataset Name**: Mobile Phone Classification Dataset

**Source**: Kaggle / UCI Machine Learning Repository

**Dataset Characteristics**:
| Property | Value |
|----------|-------|
| Number of Instances | 1001 |
| Number of Features | 20 |
| Target Variable | Binary Classification |
| Missing Values | None |

**Feature Description**:
| Feature Name | Type | Description |
|--------------|------|-------------|
| id | Numeric | Unique identifier for each phone |
| battery_power | Numeric | Battery capacity in mAh |
| blue | Binary | Has Bluetooth (0/1) |
| clock_speed | Numeric | Processor clock speed in GHz |
| dual_sim | Binary | Dual SIM support (0/1) |
| fc | Numeric | Front camera megapixels |
| four_g | Binary | 4G support (0/1) |
| int_memory | Numeric | Internal memory in GB |
| m_dep | Numeric | Mobile depth in cm |
| mobile_wt | Numeric | Mobile weight in grams |
| n_cores | Numeric | Number of processor cores |
| pc | Numeric | Primary camera megapixels |
| px_height | Numeric | Pixel resolution height |
| px_width | Numeric | Pixel resolution width |
| ram | Numeric | RAM in MB |
| sc_h | Numeric | Screen height in cm |
| sc_w | Numeric | Screen width in cm |
| talk_time | Numeric | Talk time in hours |
| three_g | Binary | 3G support (0/1) |
| touch_screen | Binary | Touch screen support (0/1) |
| wifi | Binary | WiFi support (0/1) |

**Data Preprocessing Steps**:
1. Handled missing values using [method]
2. Encoded categorical variables using Label Encoding
3. Standardized numerical features for certain models (Logistic Regression, KNN)
4. Split data into training (80%) and testing (20%) sets

## ğŸ¤– Models Used

The following 6 classification models were implemented and evaluated:

1. **Logistic Regression** - Linear model for classification
2. **Decision Tree Classifier** - Tree-based interpretable model
3. **K-Nearest Neighbors (KNN)** - Instance-based learning
4. **Naive Bayes (Gaussian)** - Probabilistic classifier
5. **Random Forest (Ensemble)** - Ensemble of decision trees
6. **XGBoost (Ensemble)** - Gradient boosting ensemble

### ğŸ“ˆ Model Comparison Table

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 Score | MCC |
|---------------|----------|-----|-----------|--------|----------|-----|
| Logistic Regression | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |
| Decision Tree | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |
| K-Nearest Neighbors | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |
| Naive Bayes | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |
| Random Forest (Ensemble) | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |
| XGBoost (Ensemble) | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX | 0.XXXX |

> **Note**: Replace the `0.XXXX` values with your actual results after running the models on your chosen dataset.

### ğŸ“ Model Performance Observations

| ML Model Name | Observation about Model Performance |
|---------------|-------------------------------------|
| Logistic Regression | [e.g., "Performs well as a baseline model. Good for linearly separable data. Fast training time and interpretable coefficients."] |
| Decision Tree | [e.g., "Prone to overfitting without proper pruning. Provides interpretable decision rules. Feature importance is easily extractable."] |
| K-Nearest Neighbors | [e.g., "Sensitive to feature scaling and curse of dimensionality. Performance depends on optimal k value. Slow prediction for large datasets."] |
| Naive Bayes | [e.g., "Fast training and prediction. Works well with high-dimensional data. Assumes feature independence which may not hold."] |
| Random Forest (Ensemble) | [e.g., "Reduces overfitting compared to single decision tree. Good handling of imbalanced data. Provides robust feature importance."] |
| XGBoost (Ensemble) | [e.g., "Best performing model with highest accuracy. Handles missing values well. Requires careful hyperparameter tuning."] |

## ğŸš€ Streamlit App Features

The deployed Streamlit application includes:

1. **ğŸ“‚ Dataset Upload** - Upload CSV files for classification
2. **ğŸ” Model Selection** - Dropdown to select from 6 different models
3. **ğŸ“Š Evaluation Metrics Display** - Shows Accuracy, AUC, Precision, Recall, F1, MCC
4. **ğŸ“ˆ Confusion Matrix** - Visual confusion matrix heatmap
5. **ğŸ“‹ Classification Report** - Detailed per-class metrics
6. **ğŸ“Š Model Comparison** - Side-by-side comparison of all models
7. **ğŸŒ³ Feature Importance** - For tree-based models

## ğŸ“ Project Structure

```
project-folder/
â”‚
â”œâ”€â”€ app.py                    # Main Streamlit application
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # Project documentation
â”‚
â”œâ”€â”€ model/                    # Model files and training scripts
â”‚   â””â”€â”€ train_models.py       # Script to train all models
â”‚
â””â”€â”€ data/                     # Dataset files (optional)
    â”œâ”€â”€ data.csv              # Training dataset
    â””â”€â”€ test_data.csv         # Test dataset
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Local Installation

1. Clone the repository:
```bash
git clone https://github.com/[your-username]/[repo-name].git
cd [repo-name]
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Run the Streamlit app:
```bash
streamlit run app.py
```

5. Open your browser and navigate to `http://localhost:8501`

## ğŸŒ Deployment

### Streamlit Community Cloud Deployment

1. Push your code to GitHub
2. Go to [Streamlit Community Cloud](https://streamlit.io/cloud)
3. Sign in with your GitHub account
4. Click "New App"
5. Select your repository and branch
6. Choose `app.py` as the main file
7. Click "Deploy"

**Live App Link**: [Your Streamlit App URL]

## ğŸ“Š How to Use

1. **Upload Dataset**: Click on "Upload CSV file" in the sidebar
   - Ensure your CSV has the target variable as the last column
   - Minimum 12 features and 500 instances recommended

2. **Select Model**: Choose a classification model from the dropdown

3. **View Results**: 
   - Single Model Evaluation tab shows detailed metrics
   - Model Comparison tab compares all 6 models
   - Feature Analysis tab shows feature importance

## ğŸ“š Evaluation Metrics Explained

| Metric | Description |
|--------|-------------|
| **Accuracy** | Proportion of correct predictions |
| **AUC** | Area Under the ROC Curve - measures discrimination ability |
| **Precision** | Proportion of true positives among predicted positives |
| **Recall** | Proportion of true positives among actual positives |
| **F1 Score** | Harmonic mean of precision and recall |
| **MCC** | Matthews Correlation Coefficient - balanced measure for imbalanced data |

## ğŸ”— Links

- **GitHub Repository**: [Your GitHub Repo Link]
- **Live Streamlit App**: [Your Streamlit App Link]

## ğŸ‘¨â€ğŸ’» Author

**Name**: [Your Name]  
**Program**: M.Tech (AIML/DSE)  
**Institution**: BITS Pilani  
**Course**: Machine Learning - Assignment 2

## ğŸ“ License

This project is for educational purposes as part of the Machine Learning course at BITS Pilani.

---

*Built with â¤ï¸ using Python, Scikit-learn, and Streamlit*
